---
title: '[머신러닝] 앙상블 (Ensemble)'

categories:
  - Machine Learning
tags:
  - Machine Learning
  - ML

last_modified_at: 2020-07-31T08:06:00-05:00

use_math: true
---

# 앙상블 (Ensemble)

"Ensemble"은 "조화"를 의미합니다. 그렇다면 무엇을 조화시키는 것일까요? 

Ensemble Learning은 여러 개의 기본 모델을 활용하여 하나의 새로운 모델을 만드는 방법입니다.

특징
과적합(overfitting)이 잘 되는 모델 사용 선호 => Test 데이터에 대한 다양한 예측값을 수집하기 위함
이렇게 Overfitting이 잘되는 모델이 Tree모델이기 때문.

## 종류 
1. Bagging
2. Random Forest 
3. Boosting
_ AdaBoost, Gradient Boosting(Xgboost, CatBoost 등등)


 앙상블 기법은 동일한 학습 알고리즘을 사용해서 여러 모델을 학습하는 개념입니다. Weak learner를 결합한다면, Single learner보다 더 나은 성능을 얻을 수 있다는 아이디어입니다. Bagging 과 Boosting 이 이에 해당합니다.

동일한 학습 알고리즘을 사용하는 방법을 앙상블이라고 한다면, 서로 다른 모델을 결합하여 새로운 모델을 만들어내는 방법도 있습니다. 대표적으로 Stacking 이 있으며, 최근 Kaggle 에서 많이 소개된 바 있습니다.


● 선형회귀분석, 로지스틱 회귀분석, 의사결정나무 알고리즘 등은 하나의 최종 모형을 제시합니다. 입력변수별 회귀계수(파라미터)가 생성되거나, IF-THEN 규칙 등으로 인해 모형의 해석력이 높지만 성능이 낮다는 단점이 있습니다.
● 앙상블 방식은 단일모형이 갖는 단점을 극복하고자, 여러 개의 모형을 적합하고 결과를 종합하여 모형의 성능을 높일 수 있습니다. 다만, 하나의 최종모형을 제시하지 못하기 때문에 모형의 해석이 어렵다는 단점이 있습니다.
● 앙상블 방식에는 크게 배깅(Bagging)과 부스팅(Boosting)이 있습니다.
  
## 배깅 (Bootstraping Aggregating ; Bagging)
Bagging (Bootstrap Aggregating) 모델을 다양하게 만들기 위해 데이터를 재구성

Bagging은 샘플을 여러 번 뽑아 각 모델을 학습시켜 결과를 집계(Aggregating) 하는 방법입니다. 아래의 그림을 통해 자세히 알아보겠습니다.
● 배깅은 관측치를 같은 크기의 동일한 확률로 복원추출하여, 서로 독립된 모형을 여러 개 적합하는 방식입니다.
● 부트스트래핑(Bootstraping)은 중복을 허용하는 랜덤 샘플링을 의미하는데, 이는 훈련셋을 여러 개 중복 생성하여 다수의 모형을 적합한 다음 평균을 계산합니다.
● 의사결정나무는 나무의 깊이가 깊을수록 과적합되기 쉬운 경향을 가지고 있지만 수 백, 수 천 개의 나무모형을 통합하면 과적합하는 단점을 상쇄시킬 수 있습니다.
○ 의사결정나무 알고리즘으로 적합한 모형은 편향은 작지만 분산이 커서 과적합하게 됩니다.
 
  분산(Variance)과 편향(Bias)
● 오차(error)는 분산과 편향이라는 두 가지 요소로 나눌 수 있습니다.
○ MSE=분산+편향2
○ 편향은 과소적합(underfitting)된
모형에서 발견되는 편차의 제곱입니다.
○ 분산은 과적합(overfitting)된 모형에서 발견되는데 훈련셋을 최대로 학습하여 모형 적합에 사용되지 않은(unseen) 데이터에 대해 발생하는 분산입니다.

 분산과 편향 (계속)
● 의사결정나무는 나무가 성장함에 따라 모형이 복잡해집니다.
● 모형 초기에 과소적합된 상태에서는 편향이 크고 분산은 작지만, 반대로 모형이 과적합되면 편향은 감소하고 분산은 커집니다.
● 따라서 의사결정나무는 편향은 작고
분산은 큰 모형이라 할 수 있습니다.
 
  랜덤 포레스트
● 랜덤 포레스트는 배깅 방식을 이용한 알고리즘입니다. 나무모형 적합에 사용되는 훈련셋이 부트스트래핑 방식으로 생성된 점에서는 같습니다. 다른 점은, 배깅이 모든 입력변수를 사용하는 반면, 랜덤 포레스트는 임의 선택 방식을 택합니다.
● 배깅이 모든 입력변수를 사용하기 때문에, 기대했던 만큼 모형의 성능이 향상되지 않습니다. 왜냐하면, 주어진 데이터셋에서 가장 중요한 역할을 하는 입력변수가 모든 모형에서 비슷한 역할을 할 것이기 때문입니다.
● 랜덤 포레스트는 입력변수도 임의로 선택하기 때문에 주목받지 못한 입력변수들도 여러 모형에서 재평가받을 수 있습니다. 이런 점이 뛰어난 성능을 가능하게 합니다.
  

## 부스팅 (Boosting)
● 부스팅은 배깅을 개선한 알고리즘입니다. 배깅처럼 여러 개의 모형을 적합한다는 점에서 같지만, 부스팅은 레코드를 복원추출하는 대신 이전 모형에서 오분류된 레코드에 대해 추출될 확률을 더 크게 조정합니다.
● 부스팅은 약한 분류기(weak classifiers) 여러 개를 적합한 다음, 결과를 통합할 때 조정상수(α)만큼 가중치를 곱한 후 합한 강한 분류기(strong classifier)를 최종 모형으로 제시합니다.
○ 조정상수(α)는 오분류율과 반비례하는 값으로 설정됩니다.
● 에이다부스트(AdaBoost)는 Adaptive Boosting을 줄인 것입니다.

## 참고자료


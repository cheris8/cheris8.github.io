---
title: '[NLP] BERT'

categories:
  - Deep Learning
tags:
  - Deep Learning
  - Natural Language Processing

last_modified_at: 2020-11-15T08:06:00-05:00

classes: wide
use_math: true
---

이 글은 원 논문인 [BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding](https://arxiv.org/abs/1810.04805)을 공부한 기록입니다.


## 1. Introduction

Language model pre-training 은 많은 자연어 처리 task에서 성능 향상을 보여왔다.

- senetence-level tasks: natural language inference, paraphrasing : 문장들을 전체적으로 분석하여 문장들 간 관계를 예측하는 것이 목적
- token-level tasks : named entity recognition, question answering : 토큰 수준에서 fine-grained output을 생성하는 것이 목적
토큰 수준에서 세분화 된 출력을 생성하려면 모델이 필요합니다.

down-stream tasks에 pre-trained language representation을 적용하는 기존의 2가지 방법

- feature-based approach
- fine-tuning approach

feature-based approach
- ELMo
- pre-trained representation을 포함하는 task-specific architecture를 추가적인 feature로서 사용
기능 기반 접근 방식은 추가 기능으로 사전 훈련 된 표현을 포함하는 작업 별 아키텍처를 사용합니다.

fine-tuning approach
- Generative Pre-trained Transformer (OpenAI GPT)
- 최소한의 task-specific parameter들을 도입하고, 모든 pre-trained parameter들을 fine-tuning함으로써 downstream tasks에 대하여 훈련

두 접근법은 pre-training 동안 같은 목적 함수를 공유합니다. 
두 접근법은 일반적인 언어 표현(general language representation)을 학습하기 위해 unidirectional language model(일방향 언어 모델)을 사용합니다.

가장 큰 한계는 표준 언어 모델(standard laugnage models)이 단방향/일방향(unidirectional)이라는 것입니다. 그리고 이는 pre-training 동안 사용될 수 있는 아키텍처의 선택을 제한합니다.

우리는 BERT : Bidirectional Encoder Representation from Transformers을 제안하여 fine-tuning based approach를 개선합니다.

BERT는 "masked language model" (MLM)을 pre-training objective로 사용함으로써 이전에 언급한 unidirectionality constraint(일방향 제약?)를 완화합니다. Masked language model은 입력(input)에서 일부 토큰을 무작위로/랜덤하게 마스킹 하고, 그리고 objective는 오직 그것의 컨텍스트(context)에만 기반하여 마스킹 된 단어의 original vocabulary id를 예측하는 것입니다.
 
left-to-right language model pre-trainig과는 달리, MLM objective는 표현(representation)이 왼쪽 컨텍스트와 오른쪽 컨텍스트를 융합할 수 있도록 합니다. /하여/ 깊은(deep) bidirectional Transformer를 pre-train하는 것을 가능하게 합니다.

우리는 text-pair representation을 공동으로(jointly) pre-train하는 "next sentence prediction" task를 사용합니다.

## 2. Related Work

pre-training general language representation의 긴 역사

### 2.1 Unsupervised Feature-based Approaches

### 2.2 Unsupervised Fine-tuning Approaches

### 2.3 Transfer Learning from Supervised Data

## 3. BERT

이 섹션에서는 BERT 및 자세한 구현을 소개합니다. 

우리의 framework에는 pre-training과 fine-tuning 두 단계가 있습니다.

pre-training하는 동안, 모델은 여러 pre-training task에 대하여 unlabeled data로 훈련됩니다.

fine-tuning을 위해, BERT 모델은 먼저 pre-trained parameter로 초기화되고, 모든 파라미터들은 downstream task의 labeled data를 사용하여 fine-tuned 됩니다. 각각의 downstream task는 비록 그들이 동일한 pre-trained parameter들로 초기화 되었다고 하더라도, 개별적인 fine-tuned 모델을 갖습니다.

// 그림 1의 question-answering exmaple을 기준으로 이 섹션에서 설명 진행

BERT의 특징은 서로 다른 task에 대하여 BERT의 통합된(unified) 아키텍처입니다. pre-trained 아키텍처와 최종 downstream 아키텍처 사이에는 최소한의 차이가 있습니다.

**Model Architecture**

BERT의 모델 아키텍처는 multi-layer bidirectional Transformer encoder입니다.

- $L$ : number of layers = number of Transformer blocks
- $H$ : hidden size
- $A$ : number of self-attention heads

- $BERT_{BASE}$ : L=12, H=768, A=12, Total Parameters=110M
- $BERT_{LARGE}$ : L=24, H=1024, A=16, Total Parameters=340M

**Input/Output Representations**

BERT가 다양한 down-stream tasks를 처리할 수 ​​있도록, 입력 표현(input representation)은 한 문장(a single sentence)과 한 쌍의 문장(a pair of sentences) (예 : ⟨질문, 답변⟩) 둘다를 하나의 토큰 시퀀스로 명확하게 나타낼 수 있습니다. 이때 "문장(sentence)"은 실제 언어 문장이 아니라 연속 텍스트의 임의적인 범위일 수 있습니다. "시퀀스(sequence)"는 BERT의 입력 토큰 시퀀스(input token sequence)를 의미하는데, 한 문장이거나 함께 묶인 두 문장일 수 있습니다.

우리는 30000개의 토큰 어휘(token vocabulary)로 WordPiece 임베딩(WordPiece embeddings)을 사용합니다. 모든 시퀀스의 첫번째 토큰은 항상 special classification token/특수 분류 토큰 [CLS] 입니다. 이 토큰에 해당되는 최종 hidden state는 분류 task를 위한 aggregate sequence representation/집계 시퀀스 표현으로 사용됩니다.
문장 쌍은 하나의 시퀀스로 함께 묶입니다.
우리는 두가지 방식으로 문장을 구별합니다.
먼저, 우리는 문장들을 special token [SEP]로 분리합니다. 다음으로, 우리는 모든 토큰에 해당 토큰이 문장 A 또는 문장 B에 속하는지 아닌지를 나타내는 모든 learned embedding을 추가합니다/더합니다.

- $E$ : input embedding
- $C$ : final hidden vector of the speicla [CLS] token $C \in R^H$
- $T_i$ : final hidden vector for the $i^{th}$ input token $T_i \in R^H$

토큰이 주어지면, 토큰의 input representation은 상응하는 token embedding, segment embedding, position embeddings를 합하여 구성됩니다.

### 3.1 Pre-training BERT

우리는 두개의 unsupervised task를 사용하여 BERT를 pre-train합니다.

**Masked LM**



**Next Sentence Prediction (NSP)**


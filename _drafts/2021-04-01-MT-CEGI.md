---
title: '[Machine Reasoning] CEGI '

categories:
  - Deep Learning
tags:
  - Natural Language Processing
  - Machine Reasoning

last_modified_at: 2020-10-04T08:06:00-05:00

classes: wide
use_math: true
---

이 글은 LSTM의 개념, 구조, 순전파, 역전파, 그리고 파이토치를 활용한 LSTM 구현에 관한 기록입니다.

원 논문
[](https://arxiv.org/pdf/2005.05240.pdf)


## 1. Introduction

## 2. Related Work



## 3. Task Definition

multi-choice reading comprehension에서, 우리는 t개의 토큰을 가지고 있는 paragraph $\mathbf{P}$, n개의 토큰을 가지고 있는 question $\mathbf{Q}$, 그리고 m개의 candidate options $\mathbf{O}$를 받습니다.
각각의 candidate option $\mathbf{O}_i$은 h개의 토큰을 가지고 있는 텍스트 $\mathbf{O}_i$입니다.

- $\mathbf{P} = [p_1, p_2, \cdots, p_t]$
- $\mathbf{Q} = [q_1, q_2, \cdots, q_n]$
- $\mathbf{O} = \left\\\{ \mathbf{O}_1, \mathbf{O}_2, \cdots, \mathbf{O}_m \right\\\}$
  - $\mathbf{O}_i = [o_1, o_2, \cdots, o_h]$

목표는 candidate option set에서 정답 $\mathbf{A}$를 선택하는 것입니다.

편의상 $\mathbf{\chi}$를 하나의 데이터 샘플, $\mathbf{y}$를 one-hot label로 표기합니다.

- $\mathbf{\chi} = \left\\\{ \mathbf{P}, \mathbf{Q}, \mathbf{O} \right\\\}$ : one data sample
- $\mathbf{y} = [y_1, y_2, \cdots, y_m]$ : one-hot label
  - each sacle $y_i = \mathbf{1} ( \mathbf{Q}_i=\mathbf{A} )$ is an indicator function

training 단계에서, 우리는 $N$개의 set $(\mathbf{\chi}, \mathbf{y})^{N}$을 입력받는데, 목표는 모델에 $f : \mathbf{\chi} \rightarrow \mathbf{y}$를 학습시키는 것입니다.

test 단계에서, 우리는 test smaples $\mathbf{\chi}^{test}$가 주어질 때 $\mathbf{y}^{test}$를 예측해야 합니다.



## 4. Methodology

## 4.1 Evidence Generation
### 4.1.1 Textual Evidence Generator

### 4.1.2 Factual Evidence Generator

## 4.2 Model Learnig with Contextual Commonsense Reasoning

## 참고문헌

[Commonsense Evidence Generation and Injection in Reading Comprehension](https://arxiv.org/pdf/2005.05240.pdf)


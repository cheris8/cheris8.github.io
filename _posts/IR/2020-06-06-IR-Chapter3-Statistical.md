---
title: '[정보검색] 제3장 텍스트의 자동색인 - 통계적 기법'

categories:
  - Informatoin Retrieval
tags:
  - Informatoin Retrieval

last_modified_at: 2020-06-06T08:06:00-05:00

classes: wide
use_math: true
---

이 글은 정영미 교수님의 [정보검색연구](https://www.aladin.co.kr/shop/wproduct.aspx?ItemId=17330455)를 바탕으로 연세대학교 문성빈 교수님의 수업을 공부한 기록입니다.

## 0. 개요

### Luhn (1957)

- 텍스트에 출현한 단어들의 통계적 특성에 의해 색인어를 선정
- 가설
    - 문헌에 출현한 단어들은 문헌의 내용 분석을 위해 사용될 수 있다.
    - 단어의 출현 빈도가 단어의 주제어로서의 중요성을 측정하는 기준이 된다.
- 최고 한계 빈도와 최저 한계 빈도 안에 속하는 중간 빈도의 단어들이 문헌 내용의 식별력이 크므로 중간빈도어를 색인어로 선정
    - 고빈도의 단어는 너무 일반적인 단어이므로 주제어로서의 가치 X (예:- 기능어)
    - 저빈도의 단어는 주제어로서의 의미가 X (예: 고유명사, 축약어)
- <그림 3.1> 단어의 빈도와 문헌식별력과의 관계 : 정규분포

### Zipf’s Law

- 단어의 Frequency * 단어의 Rank = Constant : 일정
    - 1000번 * 1위 = 1000
    - 500번 * 2위 = 1000
    - 350번 * 3위 = 1050
    - 250번 * 4위 = 1000
    - 200번 * 5위 = 1000
- 단어의 출현빈도 순위와 단어의 문헌 식별력은 반비례
    - 따라서 중간 빈도의 단어들이 good

### 통계적 기법

- 단어의 출현빈도에 근거하여 주제어로서의 중요도를 측정한 다음 색인어를 선정하는 색인 기법
- 주제어로서의 중요성을 측정하는 방법

1. 출현빈도를 직접 이용하는 방법
    - 단어의 빈도 산출 방식에 따라 단순빈도와 정규화빈도 사용
2. 출현빈도나 출현확률에 근거하여 색인어로서의 가치를 측정하는 방법
    - 단어의 문헌분리값 (Term Discrimination Value)
    - 신호량가중치 (Signal Weight)
    - 적합성 가중치 (Relevance Weight)
3. 단어의 출현빈도에 따른 확률분포를 이용하는 방법
    - 포아송 분포 모형
    - 2-포아송 분포 모형
    - 점유 분포에 기반한 단어집중도 모형

### 색인어 선정 기준으로 출현빈도를 직접 이용하는 통계적 기법의 처리 과정

1. 텍스트를 구성하는 각 단어들을 분리한 후 불용어 리스트와 대조하여 비주제어를 제거
    - 이떄 제거되는 불용어들은 대개 고빈도 기능어들
2. 나머지 단어들을 그대로 또는 어간/어근 형태로 변환하여 각 단어의 출현빈도를 산출
3. 각 단어를 출현빈도 가중치 순으로 배열한 후 일정한 기준(threshold) 이상의 값을 갖는 단어를 색인어로 선정
4. 검색 결과를 순위화하는 검색 모형에서는 색인어에 가중치를 부여하여 검색 시 검색 문헌의 적합성 값을 산출하는 데에 사용

## 1. 단순빈도와 정규화빈도 가중치

### 단순빈도의 종류

- 대부분의 용어 가중치는 아래의 세가지 빈도로부터 산출

1. 단어빈도 (Term Frequency ; TF)
    - 색인 대상이 되는 각 문헌 $D_i$에 단어 $k$가 출현한 횟수 : $tf_{ik}$
    - 특정 문헌에서 특정 단어가 출현한 횟수
    - = 문헌 내 단어빈도 = 문헌 내 빈도 (within-document frequency)
2. 문헌빈도 (Document Frequency ; DF)
    - 단어 $k$가 출현한 문헌의 수
    - 전체 문헌집단에서 특정 단어가 출현한 문헌의 수
3. 장서빈도 (Collection Frequency ; CF)
    - 단어 $k$가 전체 문헌집단에 출현한 총 빈도
    - 전체 문헌집단에서 특정 단어가 출현한 횟수
- 이원 색인 (Binary Indexing)
    - 가중치 없이 색인어를 선택하는 것
- 가중치 색인 (Weighted Indexing)
    - 색인어에 가중치를 부여하는 것

### 정규화빈도

- 단어빈도를 문헌빈도, 장서빈도, 텍스트 길이 등에 의해 정규화 한 빈도
    - 출현빈도를 정규화하지 않은 단순빈도는 문헌집단의 크기, 분석 대상 텍스트의 길이, 단어의 일반적인 사용 빈도 등을 고려하지 않은- 것이기 때문
- 역문헌빈도 (Inverse Document Frequency ; IDF)
    - 단어빈도에 곱해지는 문헌빈도의 역의 값
    - $IDF = \log \frac{N}{DF_i}$
    - $N$ : 전체 문헌집단의 문헌 총 수
    - $DF_i$ : 단어 i가 출현한 문헌 수 : 문헌빈도
- TF-IDF 가중치
    - 단어빈도를 역문헌빈도에 의해 정규화 한 가중치
    - $TF-IDF = TF \times IDF$
    - 즉 전체 문헌집단에서는 적게 출현하면서 특정 문헌에서는 높게- 출현하는 것이 good
    - 검색 실험에서 용어 가중치로 가장 많이 사용

### 색인어 = 용어 가중치 구성 요소

- 색인어 가중치는 세가지 요소의 결합으로 구성
	1. 단어빈도 (TF) 요소 : b / n / a / l
	2. 역문헌빈도 (IDF) 요소 : n / t
	3. 문헌길이 정규화 (Document Length Normalization) 요소 :  n / c / b / u
- 용어 가중치는 각 요소를 영어 알파벳의 한 문자로 기호화하여 집합하는 코드체계에 의해 간단히 표현

|요소|유형|코드|값 / 공식|
|---|---|---|-------|
|단어빈도 요소|이진 TF|b|1 or 0|
|단어빈도 요소|단순 TF|n|실제 출현빈도|
|단어빈도 요소|로그 TF|l|$1 + \log tf$|
|단어빈도 요소|보정 TF|a|$\left( 1 - b \right) + b \left( \frac{tf}{max_{tf}} \right)$|
|역문헌빈도 요소|사용 O|t|$ \log⁡ \frac{N}{df} $|
|역문헌빈도 요소|사용 X|n||
|문헌길이 정규화 요소|코사인 정규화|c|$w \ norm = \frac{w_i}{\sqrt{w_1^2 + w_2^2 + \cdots + w_t^2}}$|
|문헌길이 정규화 요소|최대_TF 정규화|a|$\frac{w_i}{문헌 \ 벡터를 \ 구성하는 \ 각 \ 용어 \ 출현빈도 \ 중 \ 최대값}$|
|문헌길이 정규화 요소|바이트 크기 정규화|b|
|문헌길이 정규화 요소|피벗 고유단어 정규화|u|
|문헌길이 정규화 요소|사용 X|n|

- 코드체계는 질의어 가중치를 표현할 때도 사용

### 단어빈도 (TF) 요소

- 단어빈도 유형

1. 이진 (binary) TF : 코드 b
	- 1 or 0
	- 이원 색인 즉 단어의 문헌 내 출현여부만을 표현하는 경우 사용
2. 단순 TF : 코드 n
	- 실제 출현빈도
3. 로그 TF : 코드 l
	- $1 + \log⁡ tf$
	- 대규모 문헌집단에서 검색문헌들의 순위화에 있어 가장 좋은 성능
4. 보정 TF : 코드 a
	- $\left( 1 - b \right) + b \frac{tf}{max_tf}$
	- 파라미터 $b$ 값으로 0.5 혹은 0.6 사용
	- 단어빈도를 문헌 내 최대빈도로 정규화 한 효과

### 역문헌빈도 (IDF) 요소

- 역문헌빈도 요소를 사용할 경우 코드는 t
	- btn : 이진 TF * IDF
	- ntn : 단순 TF * IDF
	- ltn : 로그 TF * IDF
	- atn : 보정 TF * IDF
- $ IDF = \log \frac{N}{df} $
	- $N$ : 문헌집단 내 전체 문헌 수
- 모든 문헌에서 출현한 단어의 경우 $df = N$이 되어 가중치 값이 0이 되므로 $IDF = 1 + \log \frac{N}{df}$을 사용하여 값의 범위 조정 가능
- 역문헌빈도는 문헌빈도가 낮은 단어 즉 적은 수의 문헌에 출현한 단어에 높은 중요도를 부여하는 것
	- 많은 문헌에 출현하는 단어는 문헌들을 식별하는 능력이 낮다는 가설에 기초
- 즉 역문헌빈도는 하나의 문헌에서가 아니라 전체 문헌 집단 내에서 특정한 단어가 갖는 문헌 식별 능력을 측정하는 가중치

### 문헌길이 정규화 요소

- 단어빈도 * 역문헌빈도 로 일차적인 정규화 이후
- 문헌길이에 의한 이차적인 정규화가 필요한 이유
	- “일반적으로 긴 문헌의 경우 단어의 출현빈도가 높아지므로 긴 문헌에 출현한 단어들의 가중치가 평균적으로 커지고 이로 인해 질의와의 유사도가 커진다.”
	- “긴 문헌은 더 많은 수의 고유한 단어를 포함하므로 긴 문헌이 짧은 문헌보다 질의와의 유사도가 커지게 되고 따라서 긴 문헌이 검색될 확률이 높다.”
- 문헌길이 정규화 방법
	1. 코사인 정규화 (Cosine Normalization) : 코드 c
	2. 최대_TF 정규화 (Maximum TF Normalization) : 코드 a
	3. 바이트 크기 정규화 (Byte Size Normalization) : 코드 b
	4. 피벗 고유단어 정규화 (Pivoted Unique Normalization) : 코드 u
    5. 정규화 요소가 없을 경우 : 코드 n
- 코사인 정규화 
	- 해당 용어 가중치 / 문헌벡터를 구성하는 각 용어 가중치의 자승을 더한 값의 제곤근
	- $w \ norm = \frac{w_i}{\sqrt{w_1^2 + w_2^2 + \cdots + w_t^2}}$
- 최대_TF 정규화
	- $\frac{해당 \ 용어 \ 출현빈도}{문헌벡터를 \ 구성하는 \ 각 \ 용어 \ 출현빈도 \ 중 \ 최대값}$
- 피벗 정규화 (Pivoted Normalization ; PN)
	- 문헌길이 정규화로 인해 긴 문헌의 검색 확률이 지나치게 낮아지는 점을 보완하기 위한 것
	- 코사인 정규화를 적용할 경우 짧은 길이의 문헌은 긴 길이의 문헌에 비해 질의에 대한 실제 적합 확률보다 검색확률이 크다는 점을 보완하기 위한 것
	- 짧은 길이의 문헌은 부적합 문헌일 경우에도 검색될 가능성이 더 크다는 점을 보완하기 위한 것
	- 피벗 (pivot) : 문헌의 길이를 x축으로 하고 검색확률과 적합확률을 y축으로 하여 두 개의 확률 곡선을 그렸을 때 적합확률과 검색확률이 똑같아지는 x축의 중간 지점
	- 문헌길이 정규화를 적용하였을 경우 길이가 긴 문헌은 실제 적합확률에 비해 검색확률이 낮아지는 경향이 있으므로
	- 피벗 지점 이후에는 적합확률이 곡선이 검색확률 곡선의 위쪽에 위치
	- 이렇게 검색확률과 적합확률이 차이가 나는 부분을 교정하기 위한 것이 피벗 정규화

### 예제 

**색인어 가중치 코드 = BTC : 이진 TF & IDF & 코사인 정규화**

||t1|t2|t3|t4|t5|t6|
|-|-|-|-|-|-|-|
|D1|0|2|1|0|1|0|
|D2|2|0|0|1|2|0|
|D3|2|1|0|0|0|0|
|D4|0|0|2|0|0|2|

|B|t1|t2|t3|t4|t5|t6|
|-|-|-|-|-|-|-|
|D1|0|1|1|0|1|0|
|D2|1|0|0|1|1|0|
|D3|1|1|0|0|0|0|
|D4|0|0|1|0|0|1|

||t1|t2|t3|t4|t5|t6|
|-|-|-|-|-|-|-|
|IDF|$\log \frac{4}{2}$|$\log \frac{4}{2}$|$\log \frac{4}{2}$|$\log \frac{4}{1}$|$\log \frac{4}{2}$|$\log \frac{4}{1}$|

|BT|t1|t2|t3|t4|t5|t6|
|-|-|-|-|-|-|-|
|D1|0|1|1|0|1|0|
|D2|1|0|0|2|1|0|
|D3|1|1|0|0|0|0|
|D4|0|0|1|0|0|2|

|BTC|t1|t2|t3|t4|t5|t6|
|-|-|-|-|-|-|-|
|D1|0|$\frac{1}{\sqrt{3}}$|$\frac{1}{\sqrt{3}}$|0|$\frac{1}{\sqrt{3}}$|0|
|D2|$\frac{1}{\sqrt{6}}$|0|0|$\frac{2}{\sqrt{6}}$|$\frac{1}{\sqrt{6}}$|0|
|D3|$\frac{1}{\sqrt{2}}$|$\frac{1}{\sqrt{2}}$|0|0|0|0|
|D4|0|0|$\frac{1}{\sqrt{5}}$|0|0|$\frac{2}{\sqrt{5}}$|

**색인어 가중치 코드 = NTC : 단순 TF & IDF & 코사인 정규화**

||t1|t2|t3|t4|t5|t6|
|-|-|-|-|-|-|-|
|D1|0|2|3|0|1|0|
|D2|0|0|0|1|2|0|
|D3|3|1|0|0|0|0|
|D4|0|0|2|0|0|2|

|N|t1|t2|t3|t4|t5|t6|
|-|-|-|-|-|-|-|
|D1|0|2|3|0|1|0|
|D2|0|0|0|1|2|0|
|D3|3|1|0|0|0|0|
|D4|0|0|2|0|0|2|

||t1|t2|t3|t4|t5|t6|
|-|-|-|-|-|-|-|
|IDF|$\log \frac{4}{1}$|$\log \frac{4}{2}$|$\log \frac{4}{2}$|$\log \frac{4}{1}$|$\log \frac{4}{2}$|$\log \frac{4}{1}$|

|NT|t1|t2|t3|t4|t5|t6|
|-|-|-|-|-|-|-|
|D1|$0 * \log \frac{4}{1}$|$1 * \log \frac{4}{2}$|$1 * \log \frac{4}{2}$|$0 * \log \frac{4}{1}$|$1 * \log \frac{4}{2}$|$0 * \log \frac{4}{1}$|
|D2|$1 * \log \frac{4}{1}$|$0 * \log \frac{4}{2}$|$0 * \log \frac{4}{2}$|$1 * \log \frac{4}{1}$|$1 * \log \frac{4}{2}$|$0 * \log \frac{4}{1}$|
|D3|$1 * \log \frac{4}{1}$|$1 * \log \frac{4}{2}$|$0 * \log \frac{4}{2}$|$0 * \log \frac{4}{1}$|$0 * \log \frac{4}{2}$|$0 * \log \frac{4}{1}$|
|D4|$0 * \log \frac{4}{1}$|$0 * \log \frac{4}{2}$|$1 * \log \frac{4}{2}$|$0 * \log \frac{4}{1}$|$0 * \log \frac{4}{2}$|$1 * \log \frac{4}{1}$|

|NTC|t1|t2|t3|t4|t5|t6|
|-|-|-|-|-|-|-|
|D1|$\frac{0}{\sqrt{14}}$|$\frac{2}{\sqrt{14}}$|$\frac{3}{\sqrt{14}}$|$\frac{0}{\sqrt{14}}$|$\frac{1}{\sqrt{14}}$|$\frac{0}{\sqrt{14}}$|
|D2|$\frac{0}{\sqrt{8}}$|$\frac{0}{\sqrt{8}}$|$\frac{0}{\sqrt{8}}$|$\frac{2}{\sqrt{8}}$|$\frac{2}{\sqrt{8}}$|$\frac{0}{\sqrt{8}}$|
|D3|$\frac{6}{\sqrt{37}}$|$\frac{1}{\sqrt{37}}$|$\frac{0}{\sqrt{37}}$|$\frac{0}{\sqrt{37}}$|$\frac{0}{\sqrt{37}}$|$\frac{0}{\sqrt{37}}$|
|D4|$\frac{0}{\sqrt{20}}$|$\frac{0}{\sqrt{20}}$|$\frac{2}{\sqrt{20}}$|$\frac{0}{\sqrt{20}}$|$\frac{0}{\sqrt{20}}$|$\frac{4}{\sqrt{20}}$|

## 2. 단어의 문헌분리값

### 단어의 문헌분리값 (Term Discrimination Value)

- 한 문헌집단 속에서 특정한 단어가 상호 관련 없는 문헌들을 분리시키는 능력을 측정한 것
	- "좋은 색인어는 문헌집단에서 서로 주제가 다른 문헌들을 가능한 한 분리시킨다."
	- "나쁜 색인어는 문헌집단에서 서로 주제가 다른 문헌들을 무리짓게 한다."
- 좋은 색인어일수록 문헌집단의 밀도를 낮춤으로써 즉 서로 멀리 떨어지게 함으로써
- 해당 색인어로 표현된 주제를 다루고 있는 문헌들을 그렇지 않은 이웃문헌들로부터 쉽게 구별되도록 하여
- 검색을 용이하게 한다는 것
- 특정한 단어의 문헌분리값 = 해당 단어가 색인어로 부여되기 이전과 부여된 이후의 문헌들 간 평균 유사도의 차이
	- 좋은 색인어는 이 단어를 문헌집단에서 제거했을 때 문헌들 간 평균 유사도를 증가시키는 결과 초래

### 문헌분리값 산출과정

1. 문헌 $D_i$를 용어들의 벡터로 표현
	- $D_i = (w_{i1}, w_{i2}, … w_{ik})$
	- $w_{ik}$ : 각 단어의 가중치
2. 각 문헌 쌍의 유사도 $S(D_i, D_j)$를 산출하여 평균 : 문헌집단의 평균유사도 $Q$
	- 두 문헌의 주제적 관련성은 두 문헌벡터의 유사도에 의해 측정하기 때문
	- 유사도 계산에는 다이스 계수 등이 사용
	- 평균유사도 = $\frac{1}{n(n-1)} \sum_{i=1}^n \sum_{j=1}^n S(D_i, D_j) (i \neq j)$
		- 평균유사도 : 문헌집단의 밀도
		- 평균유사도가 클수록 문헌들이 밀집
3. 문헌벡터로부터 단어 $k$를 제거한 후의 평균 유사도 $Q_k$와 단어 $k$를 색인어로 부여했을 때의 유사도 $Q$의 차이를 산출 : 단어 $k$의 문헌분리값
	- 문헌분리값 = $Q_k - Q$
	- 문헌분리값 = 양수 : $Q_k > Q$
		- 단어 k는 좋은 색인어
		- $Q_k$가 $Q$보다 크다는 것은 단어 $k$를 제거한 후 문헌이 밀집된다는 것 그리고 단어 $k$를 색인어로 부여하면 밀도를 낮출 수 있다는 것을 의미
	- 문헌분리값 = 음수 : $Q_k < Q$
		- 단어 $k$는 나쁜 색인어
	- 즉 문헌분리값이 큰 단어일수록 좋은 색인어
4. 문헌분리값을 가중치로 사용
	- 문헌분리값 자체는 한 문헌집단 내에서 특정한 단어가 색인어로 적합한지를 결정하는 기준
	- 문헌분리값을 색인어 선정 기준으로 사용하기 위한 가중치
		- $w_{ik} = TF_{ik} *$ 문헌분리값$k$

### 예제 

**문헌 - 문헌 유사도를 Dice Coefficient 공식을 이용하여 구하고, T3의 문헌분리값를 계산하고, 색인어로서의 가치를 평가하시오.**

문헌 - 용어 행렬

||t1|t2|t3|t4|t5|t6|t7|
|-|-|-|-|-|-|-|-|
|D1|1|0|0|1|1|1|0|
|D2|0|1|1|0|0|1|1|
|D3|1|1|1|1|0|0|1|
|D4|1|0|1|0|1|1|0|
|D5|0|1|1|1|0|0|1|

문헌 - 문헌 유사도 : Q3 (T3 제외 후)

||D1|D2|D3|D4|D5|
|-|-|-|-|-|-|-|
|D1|				
|D2|$\frac{2}{7}$|				
|D3|$\frac{4}{8}$|$\frac{4}{7}$|		
|D4|$\frac{6}{7}$|$\frac{2}{6}$|$\frac{2}{7}$|	
|D5|$\frac{2}{7}$|$\frac{4}{6}$|$\frac{6}{7}$|$\frac{0}{6}$|

$ Q_3 = \frac{2}{7} + \frac{4}{8} + \frac{4}{7} + \frac{6}{7} + \frac{2}{6} + \frac{2}{7} + \frac{2}{7} + \frac{4}{6} + \frac{6}{7} + \frac{0}{6} = 0.46 $

문헌 - 문헌 유사도 : Q (T3 제외 전)

||D1|D2|D3|D4|D5|
|-|-|-|-|-|-|-|
|D1|				
|D2|$\frac{2}{8}$|
|D3|$\frac{2}{9}$|$\frac{6}{11}$|			
|D4|$\frac{3}{8}$|$\frac{2}{8}$|$\frac{2}{9}$|
|D5|$\frac{1}{8}$|$\frac{3}{8}$|$\frac{4}{9}$|$\frac{1}{8}$|

$Q = \frac{2}{8} + \frac{2}{9} + \frac{6}{11} + \frac{3}{8} + \frac{2}{8} + \frac{2}{9} + \frac{1}{8} + \frac{3}{8} + \frac{4}{9} + \frac{1}{8}$

T3의 문헌분리값 $= Q_3 - Q = -0.05 < 0$ 이므로 색인어로서의 가치 X

## 3. 신호량 가중치

### Shannon

- 정보의 개념을 선택의 자유로 간주
- 이는 정보원이 생산할 수 있는 다양한 메시지로부터 하나의 메시지를 선택할 때 부여되는 선택의 자유를 의미
- 선택의 자유가 클수록 어느 메시지가 선택될 것인가에 대한 불확실성 증가
- 불확실성의 크기를 엔트로피로 측정
- 엔트로피는 선택 대상이 되는 메시지들이 갖는 평균정보량 표현
- 평균정보량 = 엔트로피 = 불확실성 = 선택의 자유
- $ H = - \sum_{i=1}^n p_i \log_2 ⁡p_i $
	- $H$ : $n$개의 메시지가 갖는 평균정보량
	- $p_i$ : 메시지 $i$의 발생 확률
	- $\log_2 ⁡p_i$ : 메시지 $i$가 갖는 정보량

### 잡음

- 잡음 = 평균정보량 = 엔트로피
- n개의 문헌으로 구성된 문헌집단에서 특정한 단어 k가 갖는 잡음
- 잡음$k$ $= - \sum_{i=1}^n \frac{tf_{ik}}{CF_k} \log_2⁡ \frac{tf_{ik}}{CF_k}$
	- $TF_{ik}$ : 각 문헌 Di 내 단어 k의 출현빈도
	- $CF_{k}$ : 전체 문헌집단 내 단어 k의 출현빈도 : 장서빈도
	- $\frac{TF_{ik}}{CF_k} = P_{ik}$ : 단어 k의 각 문헌 내 출현확률
- 잡음은 단어 k가 전체 문헌집단 내에 고르게 분포되어 있을 때 즉 각 문헌 내 출현빈도가 똑같을 때 큰 값을 갖고
- 잡음이 큰 단어는 문헌들을 식별하는 능력이 적으므로
- 색인어로 적합하지 X

### 신호량 가중치
- 잡음 공식의 역함수를 취하여 색인어로서의 가치를 결정하는 기준으로 사용
- 신호량$k$ $= \log_2 ⁡CF_k -$ 잡음$k$
	- 문헌집단에서의 총 출현빈도는 크면서 소수의 문헌에 집중적으로 출현한 단어가 큰 값의 신호량을 갖게 되어 색인어로 선정
- 신호량 가중치를 색인어 선정 기준으로 사용하기 위한 가중치
	- $w_{ik} = TF_{ik} \times$신호량$k$

### 예제

**T1의 신호량을 계산하시오.**

|	|T1|
|---|--|
|D1	|2|
|D2	|0|
|D3	|1|
|D4	|0|
|D5	|4|
|D6	|0|
|D7	|4|
|D8	|2|
|D9	|0|
|D10|1|
|D11|0|
|D12|2|
|CF|16|

잡음 $= - [(\frac{2}{16} \times \log \frac{2}{16}) + (\frac{0}{16} \times \log \frac{0}{16}) + (\frac{1}{16} \times \log \frac{1}{16}) + (\frac{0}{16} \times \log \frac{0}{16}) + (\frac{4}{16} \times \log \frac{4}{16}) + (\frac{0}{16} \times \log \frac{0}{16}) + $
$(\frac{4}{16} \times \log \frac{4}{16}) + (\frac{2}{16} \times \log \frac{2}{16}) +(\frac{0}{16} \times \log \frac{0}{16}) + (\frac{1}{16} \times \log \frac{1}{16}) + (\frac{0}{16} \times \log \frac{0}{16}) + (\frac{2}{16} \times \log \frac{2}{16})]$
$= - [(\frac{1}{16} * \log \frac{1}{16}) \times 2 + (\frac{2}{16} \times \log \frac{2}{16}) \times 3 + (\frac{4}{16} \times \log \frac{4}{16}) \times 2] = \frac{21}{8}$

신호량 $= \log_2⁡ 16 - \frac{21}{8} = \frac{11}{8}$

## 4. 적합성 가중치

### 문헌분리값 & 신호량 가중치 & 적합성 가중치

- 문헌분리값 & 신호량 가중치
	- 특정한 단어가 전체 문헌집단 내에서 출현한 빈도를 반영하여 해당 단어가 전체 문헌집단에서 색인어로서 어느 정도의 가치를 갖는지 측정
- 적합성 가중치
	- 문헌집단을 구성하는 문헌들을 특정한 질의에 대해 적합문헌과 부적합문헌으로 구별한 후 각 집합에서의 출현빈도를 반영하여 산출
	- 문헌의 적합성 정보를 이용
	- 단어의 출현빈도 뿐만 아니라 단어가 출현한 문헌의 클래스 또한 고려

### 적합성 가중치

- 질의 $Q$를 구성하는 단어 $k$에 대하여
	- $N$ = 문헌집단 내 문헌 총 수
	- $n$ = 단어 $k$를 색인어로 갖는 문헌 수 = 검색 문헌 총 수 = DF
	- $R$ = 질의 $Q$에 대한 문헌집단 내 적합 문헌 수 = 적합 문헌 총 수
	- $r$ = 질의 $Q$에 대한 적합문헌 중 색인어 $k$가 부여된 문헌 수 = 검색 적합 문헌 총 수
- 문헌의 적합성 분포


||적합 문헌|부적합 문헌|전체|
|-|-------|--------|---|
|색인어 k가 부여된 문헌|$r$|$n - r$|$n$|
|색인어 k가 부여되지 않은 문헌|$R - r$|$(N - n) - (R - r)$|$N - n$|
|전체|$R$|$N - R$|$N$|

- $W1 = \log \frac{⁡r⁄R}{n⁄N}$
	- 적합 문헌 내 분포 & 전체 문헌집단 내 분포 비교
- $W2 = \log \frac{⁡r⁄R}{(n-r)⁄(N-R)}$
	- 적합 문헌 내 분포 & 부적합 문헌 내 분포 비교
- $W3 = \log⁡ \frac{r⁄(R-r)}{n⁄(N-n)}$
	- 적합 문헌 내 분포 & 전체 문헌집단 내 분포 비교
	- 높은 성능을 보이는 가중치
- $W4 = \log ⁡\frac{r⁄(R-r)}{(n-r)⁄(N-n-R+r)}$
	- 적합 문헌 내 분포 & 부적합 문헌 내 분포 비교
	- 가장 높은 성능을 보이는 가중치

### 적합성 가중치 초기값

- 초기 검색 이전에는 적합성 정보를 얻을 수 없으므로 적합성 가중치를 산출할 수 X
- 따라서 처음에는 모든 검색어가 적합 문헌에 출현할 확률이 똑같다고 가정하고 초기 값 산출
- $W4 = \log⁡ \frac{(N-df)}{df} = \log \frac{(N-n)}{n}$
- $W4 = \log⁡ \frac{N}{n}$
	- 전체 문헌집단의 크기 $N$이 월등히 클 때 $n$ 무시 가능
	- 역문헌빈도(IDF)와 동일

## 5. 2-포아송 분포 모형

### 포아송 분포 모형 & 2-포아송 분포 모형

- 확률색인은 기본적으로 주제어와 비주제어의 분포 패턴이 다르다는 가설에 근거
- 포아송 분포 모형
	- 단어들이 문헌집단 속에 랜덤하게 분포되어 있는 현상을 설명하는 모형
	- 전체 문헌집단에 랜덤하게 분포되어 있는 단어는 색인어로 부적합
	- 비주제어(non-specialty word)의 분포 모형
- 2-포아송 분포 모형
	- 두개의 포아송 분포를 결합한 것
	- 주제어(specialty word)의 분포 모형

### 포아송 분포 모형

- 포아송 분포 모형은 문헌집단 내 총 출현빈도가 R인 단어가 A개의 문헌들 속에 랜덤하게 분포되어 있는 현상을 포아송 분포 함수로 나타낸 것
- 특정한 단어 w가 한 문헌에 k번 출현할 확률 $P(k) = \pi \frac{e^{-\lambda} \lambda^k}{k!}$
	- $\lambda$ : 단어 $w$가 문헌집단 내 각 문헌에 출현한 평균빈도 = 문헌집단 내 총 출현빈도 / 문헌집단 내 문헌 총 수 = $R / A$

### 2-포아송 분포 모형

- 문헌집단을 특정한 주제에 적합한 클래스와 부적합한 클래스로 구분하면
- 각 클래스 내에서의 단어들의 출현빈도는 포아송 분포를 따른다는 것
- 특정한 주제어 w가 한 문헌에 k번 출현할 확률 $P(k) = \pi \frac{\lambda_1^k e^{-\lambda_1}}{k!} + (1 - \pi) \frac{\lambda_2^k e^{-\lambda_2}}{k!}$
	- $\lambda_1$ : 적합문헌 클래스에서 단어 w의 평균 출현빈도
	- $\lambda_2$ : 부적합문헌 클래스에서 단어 w의 평균 출현빈도
	- $\pi$ : 적합문헌 클래스에 속하는 문헌의 비율
	- $1 - \pi$ : 부적합문헌 클래스에 속하는 문헌의 비율
	- 2-포아송 분포의 세 파라미터 $\pi$, $\lambda_1, \lambda_2$ 값은 샘플 문헌의 빈도 데이터로부터 샘플 모멘트를 계산한 다음 2-포아송 분포의 모멘트 산출 공식에 대입함으로써 추정할 수 O
- 두 문헌 클래스의 중복도를 측정하는 $z$ 값에 의해 색인어를 선정
- 두 문헌 클래스의 중복도 (degree of overlap) $z = \frac{(\lambda_1 - \lambda_2)}{(\sqrt{\lambda_1 + \lambda_2)}}$
	- 단어 $w$의 색인어로서의 가치는 적합문헌 클래스와 부적합문헌 클래스를 가능한 한 멀리 분리시키는 능력에 의해 판정
	- 적합문헌 클래스와 부적합문헌 클래스가 중복되는 정도가 적을수록
	- 두 클래스의 분포함수의 평균인 $\lambda_1$과 $\lambda_2$의 차이가 클수록
	- $\lambda_1$이 $\lambda_2$보다 훨씬 큰 값을 가질수록
	- $z$ 값이 클수록
	- 단어 $w$는 좋은 색인어로 판정
